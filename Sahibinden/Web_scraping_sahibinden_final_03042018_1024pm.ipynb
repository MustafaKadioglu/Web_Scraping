{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import codecs\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys \n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import unicodedata\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "#Since the scraping web site is encoded with UTF-8, we need to create UTF-8 encoded csv file\n",
    "#with codecs.open(\"result9.csv\", \"w\", \"utf-8-sig\") as f:\n",
    "\n",
    "# to write the header of the file, at the end of the header we added the date1 column to figure out how long\n",
    "# did it take to sell a car if it is not listed anymore. The first column is the listing date if url is not found by Chrome,\n",
    "# date will be written at the end of the header in date1 column.\n",
    "\n",
    "#    f.write(u'date, brand, serial, model, year, oil_type, gear, odometer, body, hp, eng_dim, color, warranty, seller, condition,'#\n",
    "#            'price, safe, in_fea, outs_fea, mul_fea, pai_fea, rep_fea, location, \\n')\n",
    "\n",
    "#with codecs.open(\"result2.csv\", \"w\", \"utf-8-sig\") as g:\n",
    "\n",
    "# to write the header of the file, result2 file consists of the urls related to the owner and dealer list.\n",
    "# The whole list has more than 7000 urls\n",
    "    \n",
    "#    g.write('urls\\n')\n",
    "    \n",
    "#with codecs.open(\"result4.csv\", \"w\", \"utf-8-sig\") as h:\n",
    "\n",
    "# to write the header of the file. result4 file has all the urls we are gonna scrape. The issue is that the source url\n",
    "# (sahibinden) distorts the urls by adding strings at the begining of the addresses. In order to cope with this problem\n",
    "# we need to add a refining code at the beginning of the functions.\n",
    "    \n",
    "#    h.write('urls_final\\n')\n",
    "\n",
    "    \n",
    "# Chrome path address called from desktop\n",
    "\n",
    "chrome_path = r\"C:\\Users\\Mike\\Desktop\\chromedriver.exe\"    \n",
    "\n",
    "# To call the chrome driver\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n",
    "# Function for the final page scraping \n",
    "\n",
    "def finl_page(x):\n",
    "    \n",
    "# Final listing page has 50 cars, in order to call each url, a foor loop is created.\n",
    "    driver = webdriver.Chrome(chrome_path)\n",
    "    \n",
    "    for lur in x:\n",
    "\n",
    "# to call the url of final page    \n",
    "        \n",
    "        try:\n",
    "            \n",
    "            driver.get(lur)\n",
    "        \n",
    "            try:    \n",
    "                date = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[2]/span''')\n",
    "                date = date.text\n",
    "#               print date\n",
    "                \n",
    "                brand = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[3]/span''')\n",
    "                brand = brand.text\n",
    "        #       print (brand)\n",
    "\n",
    "                serial = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[4]/span''')\n",
    "                serial = serial.text\n",
    "        #       print (serial)\n",
    "\n",
    "                model = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[5]/span''')\n",
    "                model = model.text\n",
    "\n",
    "        #       print (model)\n",
    "\n",
    "                year = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[6]/span''')\n",
    "                year = year.text\n",
    "\n",
    "        #       print (year)\n",
    "\n",
    "                oil_type = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[7]/span''')\n",
    "                oil_type = oil_type.text\n",
    "\n",
    "        #       print (oil_type)\n",
    "\n",
    "                gear = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[8]/span''')\n",
    "                gear = gear.text\n",
    "\n",
    "        #       print (gear)\n",
    "\n",
    "                odometer = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[9]/span''')\n",
    "                odometer = odometer.text\n",
    "\n",
    "        #        print (odometer)\n",
    "\n",
    "                body = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[10]/span''')\n",
    "                body = body.text\n",
    "\n",
    "        #        print (body)\n",
    "\n",
    "        # Horse power\n",
    "                hp = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[11]/span''')\n",
    "                hp = hp.text\n",
    "\n",
    "        #        print (hp)\n",
    "\n",
    "        # Engine dimension\n",
    "                eng_dim = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[12]/span''')\n",
    "                eng_dim = eng_dim.text\n",
    "\n",
    "        #        print (eng_dim)\n",
    "\n",
    "                color = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[14]/span''')\n",
    "                color = color.text\n",
    "\n",
    "        #        print (color)\n",
    "\n",
    "                warranty = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[15]/span''')\n",
    "                warranty = warranty.text\n",
    "\n",
    "        #        print (warranty)\n",
    "\n",
    "        # owner or dealer\n",
    "\n",
    "                seller = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[17]/span''')\n",
    "                seller = seller.text\n",
    "\n",
    "        #        print (seller)\n",
    "\n",
    "        # Condition, brand new or second hand\n",
    "\n",
    "                condition = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/ul/li[19]/span''')\n",
    "                condition = condition.text\n",
    "\n",
    "        #        print (condition)\n",
    "\n",
    "                price = driver.find_element_by_xpath('''//*[@id=\"classifiedDetail\"]/div[1]/div[2]/div[2]/h3''')\n",
    "                price = price.text \n",
    "\n",
    "        #        print (price)\n",
    "\n",
    "        # Features for passsenger's safety\n",
    "\n",
    "                safe = ''\n",
    "                for ur in driver.find_elements_by_xpath('''//div[@id='classifiedProperties']/ul[1]/li[@class='selected']'''):\n",
    "                    ur1 = ur.text\n",
    "                    safe += ur1 +'; '\n",
    "\n",
    "        #        print (safe)\n",
    "\n",
    "        # inner feature\n",
    "\n",
    "                in_fea = ''\n",
    "                for ins in driver.find_elements_by_xpath('''//div[@id='classifiedProperties']/ul[2]/li[@class='selected']'''):\n",
    "                    ins1 = ins.text\n",
    "                    in_fea += ins1 +'; '\n",
    "\n",
    "        #        print (in_fea)\n",
    "\n",
    "        # Outer features\n",
    "\n",
    "                outs_fea = ''\n",
    "                for outs in driver.find_elements_by_xpath('''//div[@id='classifiedProperties']/ul[3]/li[@class='selected']'''):\n",
    "                    out1 = outs.text\n",
    "                    outs_fea += out1 +'; ' \n",
    "\n",
    "        #        print (outs_fea)\n",
    "\n",
    "        # Multimedia features \n",
    "\n",
    "                mul_fea = ''\n",
    "                for mults in driver.find_elements_by_xpath('''//div[@id='classifiedProperties']/ul[4]/li[@class='selected']'''):\n",
    "                    mul = mults.text\n",
    "                    mul_fea += mul +'; '\n",
    "\n",
    "        #        print (mul_fea)\n",
    "\n",
    "        # Painted parts\n",
    "\n",
    "                pai_fea = ''\n",
    "                for pai in driver.find_elements_by_xpath('''//div[@class='classified-pair custom-area ']/ul[1]/li[@class='selected']'''):\n",
    "                    pain = pai.text\n",
    "                    pai_fea += pain +'; '\n",
    "\n",
    "        #        print (pai_fea)\n",
    "\n",
    "        # Replaced parts\n",
    "\n",
    "                rep_fea = ''\n",
    "                for rep in driver.find_elements_by_xpath('''//div[@class='classified-pair custom-area']/ul[2]/li[@class='selected']'''):\n",
    "                    repa = rep.text\n",
    "                    rep_fea += rep + '; '\n",
    "\n",
    "        #        print (rep_fea)\n",
    "        \n",
    "                location = driver.find_element_by_xpath('''//div[@class='classifiedDetailContent']/div[@class='classifiedInfo ']/h2/a[1]''')\n",
    "                location = location.text\n",
    "\n",
    "# To write utf-8 data to csv file we used 'utf-8-sig' as an argument \n",
    "\n",
    "                with codecs.open (\"result9.csv\", \"a\", \"utf-8-sig\") as f:\n",
    "\n",
    "                    f.write (date + ',' + brand + ',' + serial + ',' + model + ',' + year + ',' + oil_type + ',' + gear + ',' + odometer + ',' + body + ',' + hp + ',' + eng_dim + ',' + color + ',' + warranty + ',' + seller + ',' + condition + ',' + price + ',' + safe + ',' + in_fea + ',' + outs_fea + ',' + mul_fea + ',' + pai_fea + ',' + rep_fea + ',' + location + '\\n')                \n",
    "\n",
    "            except:\n",
    "            \n",
    "                pass\n",
    "# if the url can not be found, date will be written in the date1 column.        \n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "# At the end, close the driver\n",
    "        \n",
    "    driver.close()\n",
    "        \n",
    "                                    \n",
    "\n",
    "# This function gets 50 listings from the final page, url_neww is a list of urls consisting of pages belong to the  \n",
    "# each engine type of the brands. Since 'Sahibinden' does not allow to scrape all the pages, after 20 pages, \n",
    "# we can not reach out the rest of the listings which means that 1000 commercial of each engine type at most. That\n",
    "# is the reason why we have already scraped urls for dealers and owners which increases our chance upto twice.\n",
    "\n",
    "def final_page(url_neww):\n",
    "        \n",
    "    fn_50 =[]\n",
    "    \n",
    "    for ull in url_neww:\n",
    "    \n",
    "        try:\n",
    "               \n",
    "            driver = webdriver.Chrome(chrome_path)\n",
    "            driver.get(ull)\n",
    "    \n",
    "        except Exception as e:\n",
    "            \n",
    "            print (e)\n",
    "            \n",
    "        for i in driver.find_elements_by_xpath('''//*[@id=\"searchResultsTable\"]/tbody/tr'''):\n",
    "\n",
    "            try:\n",
    "\n",
    "                c_link = i.find_element_by_xpath(\"./td[3]/a\").get_attribute('href')\n",
    "                \n",
    "                c_lnk = c_link.split()[0]\n",
    "                \n",
    "                c_lnk = c_lnk.encode('utf-8')\n",
    "                                               \n",
    "                \n",
    "                fn_50.append(c_lnk)          \n",
    "            \n",
    "# result4 file consists of final urls are to be scraped.\n",
    "\n",
    "                with codecs.open (\"result7.csv\", \"a\", \"utf-8-sig\") as h:\n",
    "    \n",
    "                    h.write (c_lnk + '\\n')\n",
    "            except:\n",
    "            \n",
    "                pass\n",
    "        \n",
    "        driver.close()\n",
    "        \n",
    "    finl_page(fn_50)\n",
    "    \n",
    "                \n",
    "# This functions calculates how many pages each engine_type of the brand has. The reason why the function \n",
    "# is a little bit complicated is that after 1000, the listing amount was depicted with '.' like 1.200 \n",
    "# Therefore by using re, I first splitted the number and concatenated it again and converted it to an integer.\n",
    "\n",
    "def find_page_tail(url_new):\n",
    "\n",
    "    url_neww = []\n",
    "    \n",
    "    for rrl in url_new:\n",
    "        \n",
    "        driver.get(rrl)\n",
    "        \n",
    "# This part of the function tries to get the amount of the listing if it is more than 1000, then it first splits the number\n",
    "# from '.' and concatenates it again and converts to the integer \n",
    "        try:\n",
    "        \n",
    "            cr_am  = driver.find_element_by_xpath('''//*[@id=\"searchResultsSearchForm\"]/div/div[4]/div[1]/div[1]/div[1]/div/span''')\n",
    "\n",
    "            cr_am = cr_am.text.encode('utf-8')\n",
    "\n",
    "            cr_am1 = re.findall(r'\\d{1,5}', cr_am)\n",
    "\n",
    "            cr_am2 = cr_am1[0]\n",
    "\n",
    "            if '.' not in cr_am:\n",
    "\n",
    "                cr_fn = int(cr_am2)\n",
    "\n",
    "            else:\n",
    "\n",
    "                cr_am3 = cr_am1[1]\n",
    "\n",
    "                cr_fn = int(cr_am2 + cr_am3)\n",
    "\n",
    "# Source web page does not allow us to reach out more than 20 pages for each engine type that is the reason  \n",
    "# we created urls which let us reach out up to 1000 cars both sold by dealer and owner \n",
    "#which means that 2000 cars at most.If the amount of the listings between 50 and 1000 then we add tail to the end of the url.   \n",
    "\n",
    "            try:\n",
    "                if cr_fn > 50 and cr_fn <= 1000:\n",
    "\n",
    "                    for i in range(0, cr_fn+49, 50):\n",
    "\n",
    "                        url_n = rrl + '?pagingOffset=' + str(i) + '&pagingSize=50'\n",
    "\n",
    "                        url_neww.append(url_n)\n",
    "\n",
    "# If the amount of the listings are more than 1000 then we are able to get up to 1000 urls.\n",
    "                elif cr_fn > 1000:\n",
    "\n",
    "                    for i in range(0, 1001, 50):\n",
    "\n",
    "                        url_n = rrl + '?pagingOffset=' + str(i) + '&pagingSize=50'\n",
    "\n",
    "                        url_neww.append(url_n)\n",
    "                        \n",
    "# otherwise there is no reason to tweak with the urls                \n",
    "                \n",
    "                else:\n",
    "\n",
    "                    url_neww.append(rrl)               \n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print (e)\n",
    "\n",
    "        except:\n",
    "                \n",
    "            url_neww.append(rrl)\n",
    "            \n",
    "        driver.close()\n",
    "            \n",
    "#    print (url_neww)\n",
    "    \n",
    "    for ii in url_neww:\n",
    "        \n",
    "        with codecs.open (\"result2.csv\", \"a\", \"utf-8-sig\") as f:\n",
    "    \n",
    "                f.write (ii + '\\n')                \n",
    "\n",
    "    final_page(url_neww)\n",
    "    \n",
    "# This function sums the urls of the cars sold by dealer and by owner.\n",
    "\n",
    "def find_car_amount(srl_l):\n",
    "\n",
    "    url_new = []\n",
    "    \n",
    "    for urll in srl_l:\n",
    "            \n",
    "        try:\n",
    "        \n",
    "            driver.get(urll)\n",
    "        \n",
    "            try:\n",
    "            \n",
    "                for ij in driver.find_elements_by_xpath('''//ul[@class='faceted-top-buttons new-toggle-menu']/li[@class='passive']'''):\n",
    "                    \n",
    "                    slo = ij.find_element_by_xpath(\"./a[@class='phdef']\").get_attribute('href')\n",
    "                    \n",
    "                    slo = slo.split()[0]\n",
    "        \n",
    "                    slo = slo.encode('utf-8')\n",
    "\n",
    "                    url_new.append(slo)\n",
    "                \n",
    "            except:\n",
    "            \n",
    "                url_new.append(urll)\n",
    "\n",
    "        except:\n",
    "            \n",
    "            pass\n",
    "      \n",
    "        driver.close()\n",
    "        \n",
    "    print (url_new)\n",
    "    \n",
    "    find_page_tail(url_new)\n",
    "\n",
    "    \n",
    "# This function returns a list consists of the urls of each brand's serials\n",
    "\n",
    "def find_serial(ccr_l):\n",
    "    \n",
    "    srl_l = [] \n",
    "    \n",
    "    for srl in ccr_l:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            driver.get(srl)\n",
    "                \n",
    "        except Exception as e:\n",
    "            \n",
    "            print (e)\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            for j in driver.find_elements_by_xpath('''//*[@id=\"searchCategoryContainer\"]/div/div/ul/li'''):\n",
    "            \n",
    "                sr_link = j.find_element_by_xpath(\"./a\").get_attribute('href')\n",
    "            \n",
    "                sr_link = sr_link.split()[0]\n",
    "        \n",
    "                sr_link = sr_link.encode('utf-8')\n",
    "                \n",
    "                srl_l.append(sr_link)\n",
    "        \n",
    "        except:\n",
    "                \n",
    "            srl_l.append(srl)\n",
    "    \n",
    "        driver.close()\n",
    "        \n",
    "    print (srl_l)\n",
    "    \n",
    "    find_car_amount(srl_l)\n",
    "\n",
    "\n",
    "# This function return the list models of each brand     \n",
    "\n",
    "def find_model(cc_lnk):\n",
    "    \n",
    "    ccr_l = [] \n",
    "    \n",
    "    for urrl in cc_lnk:\n",
    "        \n",
    "        try:\n",
    "    \n",
    "            driver.get(urrl)\n",
    "                \n",
    "        except Exception as e:\n",
    "            \n",
    "            print (e)\n",
    "            \n",
    "        try:\n",
    "            \n",
    "            for j in driver.find_elements_by_xpath('''//*[@id=\"searchCategoryContainer\"]/div/div[1]/ul/li'''):\n",
    "            \n",
    "                c1_link = j.find_element_by_xpath(\"./a\").get_attribute('href')\n",
    "            \n",
    "                c1_link = c1_link.split()[0]\n",
    "        \n",
    "                c1_link = c1_link.encode('utf-8')\n",
    "                \n",
    "                ccr_l.append(c1_link)\n",
    "        \n",
    "        except Exception as e:\n",
    "                \n",
    "                print (e)\n",
    "    \n",
    "        driver.close()\n",
    "    \n",
    "    print (ccr_l)\n",
    "    \n",
    "    find_serial(ccr_l) \n",
    "                              \n",
    "# This function returns 71 brands of the cars url list\n",
    "\n",
    "def main_func(url = 'https://www.sahibinden.com'):\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "\n",
    "        driver.find_element_by_xpath('''//*[@id=\"container\"]/div[3]/div/aside/div[1]/nav/ul[3]/li[2]/ul/li[1]/a''').click()\n",
    "\n",
    "        driver.find_element_by_xpath('''//*[@id=\"container\"]/div/div[1]/div[1]/div[2]/a''').click()\n",
    "\n",
    "        driver.find_element_by_xpath('''//*[@id=\"searchResultsSearchForm\"]/div/div[3]/div[1]/div[2]/ul/li[1]/a''').click()\n",
    "        \n",
    "    except TimeoutException as ex:\n",
    "    \n",
    "        webdriver.navigate().refresh()\n",
    "\n",
    "# to reach out 71 brands in the web page\n",
    "\n",
    "    cc_lnk = []\n",
    "    \n",
    "    for i in driver.find_elements_by_xpath('''//*[@id=\"searchCategoryContainer\"]/div/div[1]/ul/li'''):\n",
    "\n",
    "        try:\n",
    "     \n",
    "            car_link = i.find_element_by_xpath(\"./a\").get_attribute('href')\n",
    "    \n",
    "            car_link = car_link.split()[0]\n",
    "        \n",
    "            car_link = car_link.encode('utf-8')\n",
    "            \n",
    "            cc_lnk.append(car_link)\n",
    "        \n",
    "        except Exception as e:\n",
    "    \n",
    "            print (e)\n",
    "\n",
    "    print cc_lnk\n",
    "    \n",
    "    find_model(cc_lnk)\n",
    "\n",
    "    driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('result7.csv')\n",
    "x = []\n",
    "for i in df['date']:\n",
    "    i = i.replace('\\xef\\xbb\\xbf','')\n",
    "    x.append(i)\n",
    "x = x[200:]\n",
    "finl_page(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import sys\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import urllib\n",
    "from urllib import urlopen as uReq\n",
    "import codecs\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "# To create a file named 'walmart_toys.csv' and add 'product,price_was,price_save,price,star,review,promo_flag' we run this \n",
    "#code written below.Be advised that once you use with open, you do not need to close the csv file\n",
    "\n",
    "df = pd.read_csv('result7.csv')\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in df['date']:\n",
    "    i = i.replace('\\xef\\xbb\\xbf','')\n",
    "    x.append(i)\n",
    "\n",
    "#with codecs.open(\"sahibinden_final_v1.csv\", \"w\", \"utf-8-sig\") as f:\n",
    "\n",
    "#    f.write(u'date, brand, serial, model, year, oil_type, gear, odometer, body, hp, eng_dim, color, warranty, seller, condition,'#\n",
    "#            'price, safe, in_fea, outs_fea, mul_fea, pai_fea, rep_fea, location, \\n')\n",
    "\n",
    "# We are going to scrape 26 pages, that is the reason why we created a for loop with the format command.\n",
    "\n",
    "for url in x[:3]:\n",
    "\n",
    "    try:\n",
    "        \n",
    "        html = uReq(url)\n",
    "    \n",
    "        page_soup= soup(html, 'html.parser')\n",
    "    \n",
    "        containers=page_soup.findAll('div', {'class':'classifiedInfo '}) \n",
    "    \n",
    "        print containers.text\n",
    "\n",
    "    except Exception as e:\n",
    "    \n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.5",
   "language": "python",
   "name": "python3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
